# AirImpute Pro - Comprehensive CI/CD Pipeline
# Following IEEE/ACM Standards for Scientific Software Engineering
# Implements multi-stage validation, reproducibility checks, and cross-platform deployment

name: Scientific CI/CD Pipeline

on:
  push:
    branches: [main, develop, 'release/**']
    tags: ['v*']
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      deploy_environment:
        description: 'Deployment environment'
        required: false
        default: 'staging'
        type: choice
        options:
          - staging
          - production

# Ensure only one workflow runs at a time for PRs
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  RUST_BACKTRACE: full
  CARGO_TERM_COLOR: always
  NODE_VERSION: '18.19.0'
  PYTHON_VERSION: '3.11'
  RUST_VERSION: '1.75.0'
  TAURI_PRIVATE_KEY: ${{ secrets.TAURI_PRIVATE_KEY }}
  TAURI_KEY_PASSWORD: ${{ secrets.TAURI_KEY_PASSWORD }}

jobs:
  # ========================================
  # Stage 1: Code Quality & Static Analysis
  # ========================================
  code-quality:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    outputs:
      frontend-changes: ${{ steps.changes.outputs.frontend }}
      backend-changes: ${{ steps.changes.outputs.backend }}
      scientific-changes: ${{ steps.changes.outputs.scientific }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis
          
      - name: Detect Changed Files
        id: changes
        uses: dorny/paths-filter@v2
        with:
          filters: |
            frontend:
              - 'src/**'
              - 'package*.json'
              - 'tsconfig*.json'
              - 'vite.config.ts'
            backend:
              - 'src-tauri/**'
              - 'Cargo.toml'
              - 'Cargo.lock'
            scientific:
              - 'scripts/**/*.py'
              - 'tests/scientific/**'
              - 'tests/benchmarks/**'
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: ${{ env.RUST_VERSION }}
          components: rustfmt, clippy
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache Dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/
            node_modules/
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}-npm-${{ hashFiles('**/package-lock.json') }}
          
      - name: Install Dependencies
        run: |
          npm ci
          cd src-tauri && cargo fetch
          pip install ruff mypy black isort bandit safety
          
      - name: Frontend Linting & Type Checking
        if: steps.changes.outputs.frontend == 'true' || github.event_name == 'push'
        run: |
          npm run lint
          npm run type-check
          
      - name: Frontend Code Formatting Check
        if: steps.changes.outputs.frontend == 'true' || github.event_name == 'push'
        run: npm run format:check
        
      - name: Rust Linting (Clippy)
        if: steps.changes.outputs.backend == 'true' || github.event_name == 'push'
        run: |
          cd src-tauri
          cargo clippy -- -D warnings
          
      - name: Rust Formatting Check
        if: steps.changes.outputs.backend == 'true' || github.event_name == 'push'
        run: |
          cd src-tauri
          cargo fmt -- --check
          
      - name: Python Linting & Type Checking
        run: |
          ruff check scripts/ tests/
          mypy scripts/ tests/ --ignore-missing-imports
          black --check scripts/ tests/
          isort --check-only scripts/ tests/
          
      - name: Security Scanning - Frontend
        run: |
          npm audit --production
          npx snyk test --severity-threshold=high || true
          
      - name: Security Scanning - Rust
        run: |
          cd src-tauri
          cargo audit
          
      - name: Security Scanning - Python
        run: |
          bandit -r scripts/ -ll
          safety check --json
          
      - name: License Compliance Check
        run: |
          npm install -g license-checker
          license-checker --summary --production
          
      - name: Generate Code Quality Report
        if: always()
        run: |
          mkdir -p reports/code-quality
          echo "# Code Quality Report" > reports/code-quality/report.md
          echo "Generated at: $(date)" >> reports/code-quality/report.md
          echo "Commit: ${{ github.sha }}" >> reports/code-quality/report.md
          
      - name: Upload Code Quality Artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: code-quality-report
          path: reports/code-quality/

  # ========================================
  # Stage 2: Build Verification
  # ========================================
  build-verification:
    name: Build Verification - ${{ matrix.platform }}
    needs: code-quality
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform: ubuntu-latest
            rust-target: x86_64-unknown-linux-gnu
          - platform: windows-latest
            rust-target: x86_64-pc-windows-msvc
          - platform: macos-latest
            rust-target: x86_64-apple-darwin
          - platform: macos-latest
            rust-target: aarch64-apple-darwin
            
    runs-on: ${{ matrix.platform }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Build Environment
        uses: ./.github/actions/setup-build-env
        with:
          node-version: ${{ env.NODE_VERSION }}
          rust-version: ${{ env.RUST_VERSION }}
          python-version: ${{ env.PYTHON_VERSION }}
          rust-target: ${{ matrix.rust-target }}
          
      - name: Install System Dependencies (Linux)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libwebkit2gtk-4.0-dev \
            libgtk-3-dev \
            libayatana-appindicator3-dev \
            librsvg2-dev \
            libasound2-dev \
            libssl-dev \
            libpython3-dev \
            python3-numpy \
            python3-pandas \
            python3-scipy
            
      - name: Install System Dependencies (macOS)
        if: runner.os == 'macOS'
        run: |
          brew install python@${{ env.PYTHON_VERSION }}
          pip3 install numpy pandas scipy
          
      - name: Install System Dependencies (Windows)
        if: runner.os == 'Windows'
        run: |
          choco install python --version=${{ env.PYTHON_VERSION }}
          pip install numpy pandas scipy
          
      - name: Build Frontend
        run: |
          npm ci
          npm run build:frontend
          
      - name: Build Rust Backend
        run: |
          cd src-tauri
          cargo build --release --target ${{ matrix.rust-target }}
          
      - name: Verify Build Artifacts
        run: |
          # Verify frontend build
          test -d dist
          test -f dist/index.html
          
          # Verify Rust build
          cd src-tauri
          test -f target/${{ matrix.rust-target }}/release/airimpute-pro*
          
      - name: Create Build Report
        run: |
          mkdir -p reports/build
          echo "# Build Report - ${{ matrix.platform }}" > reports/build/report-${{ matrix.platform }}.md
          echo "Platform: ${{ matrix.platform }}" >> reports/build/report-${{ matrix.platform }}.md
          echo "Target: ${{ matrix.rust-target }}" >> reports/build/report-${{ matrix.platform }}.md
          echo "Build Status: Success" >> reports/build/report-${{ matrix.platform }}.md
          
      - name: Upload Build Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: build-${{ matrix.platform }}-${{ matrix.rust-target }}
          path: |
            dist/
            src-tauri/target/${{ matrix.rust-target }}/release/

  # ========================================
  # Stage 3: Comprehensive Testing
  # ========================================
  test-suite:
    name: Test Suite - ${{ matrix.test-type }}
    needs: build-verification
    strategy:
      matrix:
        test-type: [unit, integration, scientific]
        platform: [ubuntu-latest, windows-latest, macos-latest]
        exclude:
          # Scientific tests only need to run on one platform
          - test-type: scientific
            platform: windows-latest
          - test-type: scientific
            platform: macos-latest
            
    runs-on: ${{ matrix.platform }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Test Environment
        uses: ./.github/actions/setup-build-env
        with:
          node-version: ${{ env.NODE_VERSION }}
          rust-version: ${{ env.RUST_VERSION }}
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Download Build Artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-${{ matrix.platform }}-${{ contains(matrix.platform, 'ubuntu') && 'x86_64-unknown-linux-gnu' || contains(matrix.platform, 'windows') && 'x86_64-pc-windows-msvc' || 'x86_64-apple-darwin' }}
          
      - name: Run Unit Tests
        if: matrix.test-type == 'unit'
        run: |
          # Frontend unit tests
          npm test -- --coverage --reporter=json --outputFile=reports/frontend-test-results.json
          
          # Rust unit tests
          cd src-tauri
          cargo test --all-features -- --nocapture --test-threads=1
          
      - name: Run Integration Tests
        if: matrix.test-type == 'integration'
        run: |
          # Start the application in test mode
          npm run test:integration
          
      - name: Run Scientific Validation Tests
        if: matrix.test-type == 'scientific'
        run: |
          # Install scientific test dependencies
          pip install pytest pytest-cov hypothesis pandas numpy scipy scikit-learn
          
          # Run scientific validation suite
          cd tests/scientific
          pytest -v --cov=../../scripts --cov-report=xml --cov-report=html
          
          # Run accuracy benchmarks
          cd ../benchmarks
          python performance_benchmarks.py --mode=accuracy
          
      - name: Generate Test Coverage Report
        if: always()
        run: |
          mkdir -p reports/coverage
          
          # Merge coverage reports
          if [ "${{ matrix.test-type }}" = "unit" ]; then
            npx nyc report --reporter=lcov --report-dir=reports/coverage/frontend
            cd src-tauri && cargo tarpaulin --out Xml --output-dir=../reports/coverage/rust
          fi
          
      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-results-${{ matrix.test-type }}-${{ matrix.platform }}
          path: reports/
          
      - name: Upload Coverage to Codecov
        if: matrix.test-type == 'unit'
        uses: codecov/codecov-action@v3
        with:
          files: ./reports/coverage/**/*.xml
          flags: ${{ matrix.test-type }}-${{ matrix.platform }}
          fail_ci_if_error: true

  # ========================================
  # Stage 4: Scientific Reproducibility
  # ========================================
  reproducibility-validation:
    name: Scientific Reproducibility Validation
    needs: test-suite
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Scientific Environment
        run: |
          # Create isolated conda environment for reproducibility
          wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
          bash miniconda.sh -b -p $HOME/miniconda
          source "$HOME/miniconda/etc/profile.d/conda.sh"
          conda create -n airimpute python=${{ env.PYTHON_VERSION }} -y
          conda activate airimpute
          
          # Install exact versions from requirements
          pip install -r requirements-exact.txt
          
      - name: Run Reproducibility Tests
        run: |
          source "$HOME/miniconda/etc/profile.d/conda.sh"
          conda activate airimpute
          
          # Test 1: Deterministic results with fixed seeds
          python tests/reproducibility/test_determinism.py
          
          # Test 2: Cross-platform consistency
          python tests/reproducibility/test_cross_platform.py
          
          # Test 3: Version stability
          python tests/reproducibility/test_version_stability.py
          
      - name: Validate Against Published Results
        run: |
          # Download reference results from research paper
          wget https://airimpute.pro/data/reference-results.json
          
          # Run validation
          python tests/scientific/validate_against_paper.py \
            --reference=reference-results.json \
            --tolerance=0.001  # 0.1% tolerance as specified
            
      - name: Generate Reproducibility Certificate
        if: success()
        run: |
          mkdir -p reports/reproducibility
          python scripts/generate_reproducibility_cert.py \
            --output=reports/reproducibility/certificate.pdf \
            --commit=${{ github.sha }} \
            --date=$(date -u +%Y-%m-%d)
            
      - name: Upload Reproducibility Report
        uses: actions/upload-artifact@v3
        with:
          name: reproducibility-report
          path: reports/reproducibility/

  # ========================================
  # Stage 5: Performance Benchmarking
  # ========================================
  performance-benchmarks:
    name: Performance Benchmarking
    needs: test-suite
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Benchmark Environment
        uses: ./.github/actions/setup-build-env
        with:
          node-version: ${{ env.NODE_VERSION }}
          rust-version: ${{ env.RUST_VERSION }}
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install Benchmark Tools
        run: |
          # Install hyperfine for command-line benchmarking
          wget https://github.com/sharkdp/hyperfine/releases/download/v1.18.0/hyperfine_1.18.0_amd64.deb
          sudo dpkg -i hyperfine_1.18.0_amd64.deb
          
          # Install criterion for Rust benchmarks
          cargo install cargo-criterion
          
      - name: Run Rust Benchmarks
        run: |
          cd src-tauri
          cargo criterion --output-format json > ../reports/rust-benchmarks.json
          
      - name: Run Python Benchmarks
        run: |
          cd tests/benchmarks
          python performance_benchmarks.py \
            --mode=performance \
            --output=../../reports/python-benchmarks.json
            
      - name: Run End-to-End Benchmarks
        run: |
          # Build release version
          npm run build
          
          # Run benchmarks on different dataset sizes
          hyperfine --export-json reports/e2e-benchmarks.json \
            'npm run benchmark:small' \
            'npm run benchmark:medium' \
            'npm run benchmark:large'
            
      - name: Compare with Baseline
        run: |
          # Download previous benchmark results
          gh api repos/${{ github.repository }}/actions/artifacts \
            --jq '.artifacts[] | select(.name == "benchmark-baseline") | .archive_download_url' \
            | xargs -I {} gh api {} --output baseline.zip || echo "No baseline found"
            
          if [ -f baseline.zip ]; then
            unzip baseline.zip -d baseline/
            python scripts/compare_benchmarks.py \
              --current=reports/ \
              --baseline=baseline/ \
              --threshold=0.1  # 10% regression threshold
          fi
          
      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: reports/
          
      - name: Update Benchmark Baseline
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-baseline
          path: reports/

  # ========================================
  # Stage 6: Documentation Generation
  # ========================================
  documentation:
    name: Documentation Generation
    needs: code-quality
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Documentation Environment
        run: |
          # Install documentation tools
          pip install sphinx sphinx-rtd-theme sphinx-autodoc-typehints
          npm install -g typedoc @microsoft/api-extractor @microsoft/api-documenter
          cargo install cargo-doc
          
      - name: Generate API Documentation
        run: |
          # TypeScript/React API docs
          typedoc --out docs/api/frontend src/
          
          # Rust API docs
          cd src-tauri
          cargo doc --no-deps --document-private-items
          mv target/doc ../docs/api/rust
          
          # Python API docs
          cd ../
          sphinx-build -b html docs/source docs/api/python
          
      - name: Generate User Manual
        run: |
          # Convert markdown to PDF
          pip install pandoc
          pandoc README.md -o docs/user-manual.pdf \
            --pdf-engine=xelatex \
            --template=docs/templates/manual.tex
            
      - name: Generate Scientific Documentation
        run: |
          # Generate methods documentation
          python scripts/generate_methods_docs.py \
            --output=docs/scientific-methods.pdf
            
          # Generate validation report
          python scripts/generate_validation_report.py \
            --output=docs/validation-report.pdf
            
      - name: Deploy Documentation
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs

  # ========================================
  # Stage 7: Cross-Platform Package Building
  # ========================================
  build-packages:
    name: Build Release Package - ${{ matrix.platform }}
    needs: [reproducibility-validation, performance-benchmarks]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v'))
    
    strategy:
      matrix:
        include:
          - platform: ubuntu-latest
            target: AppImage
          - platform: ubuntu-latest
            target: deb
          - platform: windows-latest
            target: msi
          - platform: windows-latest
            target: portable
          - platform: macos-latest
            target: dmg
          - platform: macos-latest
            target: app
            
    runs-on: ${{ matrix.platform }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Build Environment
        uses: ./.github/actions/setup-build-env
        with:
          node-version: ${{ env.NODE_VERSION }}
          rust-version: ${{ env.RUST_VERSION }}
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install Platform Dependencies
        run: |
          if [ "${{ runner.os }}" = "Linux" ]; then
            sudo apt-get update
            sudo apt-get install -y rpm
          fi
          
      - name: Configure Code Signing
        if: matrix.platform == 'windows-latest' || matrix.platform == 'macos-latest'
        run: |
          if [ "${{ runner.os }}" = "Windows" ]; then
            echo "${{ secrets.WINDOWS_CERTIFICATE }}" | base64 -d > certificate.pfx
            echo "TAURI_WINDOWS_CERTIFICATE_PATH=$(pwd)/certificate.pfx" >> $GITHUB_ENV
            echo "TAURI_WINDOWS_CERTIFICATE_PASSWORD=${{ secrets.WINDOWS_CERTIFICATE_PASSWORD }}" >> $GITHUB_ENV
          elif [ "${{ runner.os }}" = "macOS" ]; then
            echo "${{ secrets.MACOS_CERTIFICATE }}" | base64 -d > certificate.p12
            security create-keychain -p "${{ secrets.MACOS_KEYCHAIN_PASSWORD }}" build.keychain
            security import certificate.p12 -k build.keychain -P "${{ secrets.MACOS_CERTIFICATE_PASSWORD }}" -T /usr/bin/codesign
            security set-key-partition-list -S apple-tool:,apple: -s -k "${{ secrets.MACOS_KEYCHAIN_PASSWORD }}" build.keychain
            echo "TAURI_APPLE_SIGNING_IDENTITY=${{ secrets.MACOS_SIGNING_IDENTITY }}" >> $GITHUB_ENV
          fi
          
      - name: Build Package
        run: |
          npm ci
          npm run tauri build -- --target ${{ matrix.target }}
          
      - name: Notarize macOS Package
        if: matrix.platform == 'macos-latest'
        run: |
          xcrun notarytool submit src-tauri/target/release/bundle/${{ matrix.target }}/*.dmg \
            --apple-id "${{ secrets.APPLE_ID }}" \
            --password "${{ secrets.APPLE_PASSWORD }}" \
            --team-id "${{ secrets.APPLE_TEAM_ID }}" \
            --wait
            
      - name: Verify Package Integrity
        run: |
          # Calculate checksums
          find src-tauri/target/release/bundle -type f \( -name "*.deb" -o -name "*.AppImage" -o -name "*.msi" -o -name "*.dmg" \) \
            -exec sha256sum {} \; > checksums.txt
            
      - name: Scan Package for Malware
        run: |
          # Use ClamAV for malware scanning
          if [ "${{ runner.os }}" = "Linux" ]; then
            sudo apt-get install -y clamav
            sudo freshclam
            clamscan -r src-tauri/target/release/bundle/
          fi
          
      - name: Upload Release Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: release-${{ matrix.platform }}-${{ matrix.target }}
          path: |
            src-tauri/target/release/bundle/
            checksums.txt

  # ========================================
  # Stage 8: Release and Deployment
  # ========================================
  create-release:
    name: Create Release
    needs: build-packages
    if: startsWith(github.ref, 'refs/tags/v')
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
          
      - name: Create Release Notes
        run: |
          # Generate release notes from commits
          git log --pretty=format:"- %s" $(git describe --tags --abbrev=0 HEAD^)..HEAD > release-notes.md
          
          # Add scientific validation results
          echo -e "\n\n## Scientific Validation\n" >> release-notes.md
          cat artifacts/reproducibility-report/certificate.md >> release-notes.md
          
          # Add performance benchmarks
          echo -e "\n\n## Performance Benchmarks\n" >> release-notes.md
          python scripts/format_benchmark_results.py \
            --input=artifacts/benchmark-results/ \
            --format=markdown >> release-notes.md
            
      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          body_path: release-notes.md
          files: |
            artifacts/release-*/**/*
            artifacts/reproducibility-report/certificate.pdf
          draft: false
          prerelease: ${{ contains(github.ref, '-beta') || contains(github.ref, '-rc') }}
          
      - name: Update Auto-Update Endpoint
        run: |
          # Generate update manifest
          python scripts/generate_update_manifest.py \
            --version=${{ github.ref_name }} \
            --artifacts=artifacts/ \
            --output=update-manifest.json
            
          # Deploy to update server
          curl -X POST https://api.airimpute.pro/desktop/updates/deploy \
            -H "Authorization: Bearer ${{ secrets.UPDATE_SERVER_TOKEN }}" \
            -H "Content-Type: application/json" \
            -d @update-manifest.json

  # ========================================
  # Stage 9: Post-Release Validation
  # ========================================
  post-release-validation:
    name: Post-Release Validation
    needs: create-release
    if: startsWith(github.ref, 'refs/tags/v')
    runs-on: ubuntu-latest
    
    steps:
      - name: Test Auto-Update System
        run: |
          # Download previous version
          wget https://github.com/${{ github.repository }}/releases/latest/download/AirImpute-Pro.AppImage
          
          # Run and check for update
          ./AirImpute-Pro.AppImage --check-update
          
      - name: Verify Package Accessibility
        run: |
          # Test download links
          for platform in linux windows macos; do
            curl -f -I https://github.com/${{ github.repository }}/releases/latest/download/airimpute-pro-$platform.zip
          done
          
      - name: Submit to Package Repositories
        run: |
          # Submit to Homebrew
          gh workflow run homebrew-formula-update.yml \
            --repo airimpute/homebrew-tap \
            --field version=${{ github.ref_name }}
            
          # Submit to Chocolatey
          choco push airimpute-pro.${{ github.ref_name }}.nupkg \
            --source https://push.chocolatey.org/ \
            --api-key ${{ secrets.CHOCOLATEY_API_KEY }}
            
      - name: Notify Release Channels
        run: |
          # Send notifications
          curl -X POST ${{ secrets.DISCORD_WEBHOOK }} \
            -H "Content-Type: application/json" \
            -d '{"content": "AirImpute Pro ${{ github.ref_name }} has been released! Check out the release notes: https://github.com/${{ github.repository }}/releases/latest"}'

# ========================================
# Reusable Workflow: Security Audit
# ========================================
  security-audit:
    name: Weekly Security Audit
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Run Comprehensive Security Scan
        run: |
          # OWASP Dependency Check
          docker run --rm -v $(pwd):/src owasp/dependency-check \
            --scan /src --format ALL --project "AirImpute Pro"
            
          # Trivy vulnerability scanner
          docker run --rm -v $(pwd):/src aquasec/trivy fs /src
          
          # GitLeaks secret scanning
          docker run --rm -v $(pwd):/src zricethezav/gitleaks detect --source /src
          
      - name: Create Security Report
        if: always()
        run: |
          mkdir -p reports/security
          echo "# Security Audit Report" > reports/security/audit-$(date +%Y%m%d).md
          echo "Generated: $(date)" >> reports/security/audit-$(date +%Y%m%d).md
          
      - name: Upload Security Report
        uses: actions/upload-artifact@v3
        with:
          name: security-audit-$(date +%Y%m%d)
          path: reports/security/