name: Performance Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      comparison_branch:
        description: 'Branch to compare against'
        required: false
        default: 'main'

env:
  BENCHMARK_DATA_DIR: benchmark-data
  REGRESSION_THRESHOLD: 0.1  # 10% performance regression threshold

jobs:
  rust-benchmarks:
    name: Rust Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Rust Environment
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: '1.75.0'
          
      - name: Cache Cargo Dependencies
        uses: Swatinem/rust-cache@v2
        with:
          workspaces: './src-tauri -> target'
          
      - name: Install Benchmark Tools
        run: |
          cargo install cargo-criterion
          cargo install critcmp
          
      - name: Run Benchmarks
        run: |
          cd src-tauri
          
          # Create benchmarks if they don't exist
          mkdir -p benches
          cat > benches/imputation_bench.rs << 'EOF'
          use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
          
          fn benchmark_rah_imputation(c: &mut Criterion) {
              let mut group = c.benchmark_group("rah_imputation");
              
              for size in [100, 1000, 10000, 100000].iter() {
                  group.bench_with_input(BenchmarkId::from_parameter(size), size, |b, &size| {
                      b.iter(|| {
                          // Benchmark imputation algorithm
                          black_box(size);
                      });
                  });
              }
              group.finish();
          }
          
          fn benchmark_data_loading(c: &mut Criterion) {
              c.bench_function("load_csv_1mb", |b| {
                  b.iter(|| {
                      // Benchmark CSV loading
                  });
              });
          }
          
          criterion_group!(benches, benchmark_rah_imputation, benchmark_data_loading);
          criterion_main!(benches);
          EOF
          
          # Run benchmarks
          cargo criterion --output-format quiet --message-format json > ../rust-benchmarks.json
          
          # Save benchmark results
          mkdir -p ../${{ env.BENCHMARK_DATA_DIR }}
          cp -r target/criterion/* ../${{ env.BENCHMARK_DATA_DIR }}/
          
      - name: Compare with Baseline
        if: github.event_name == 'pull_request'
        run: |
          cd src-tauri
          
          # Fetch baseline from main branch
          git fetch origin main:main
          git checkout main
          cargo criterion --output-format quiet
          
          # Compare results
          critcmp main ${{ github.sha }} > ../benchmark-comparison.txt
          
      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v3
        with:
          name: rust-benchmark-results
          path: |
            rust-benchmarks.json
            benchmark-comparison.txt
            ${{ env.BENCHMARK_DATA_DIR }}/
            
  python-benchmarks:
    name: Python Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Python Environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install Dependencies
        run: |
          pip install -r requirements.txt -r requirements-dev.txt
          pip install pytest-benchmark airspeed pyperformance memory_profiler
          
      - name: Create Benchmark Suite
        run: |
          mkdir -p tests/benchmarks
          cat > tests/benchmarks/test_performance.py << 'PYTHON'
          import pytest
          import numpy as np
          import pandas as pd
          from memory_profiler import memory_usage
          import time
          
          class BenchmarkSuite:
              """Performance benchmarks for scientific algorithms"""
              
              @pytest.mark.benchmark(group="imputation")
              def test_rah_imputation_performance(self, benchmark):
                  """Benchmark RAH imputation algorithm"""
                  data = pd.DataFrame(
                      np.random.randn(10000, 10),
                      columns=[f'var_{i}' for i in range(10)]
                  )
                  # Add missing values
                  mask = np.random.random(data.shape) < 0.2
                  data[mask] = np.nan
                  
                  def impute():
                      # Simulate imputation
                      return data.fillna(method='ffill')
                  
                  result = benchmark(impute)
                  assert result is not None
              
              @pytest.mark.benchmark(group="data_processing")
              def test_data_loading_performance(self, benchmark):
                  """Benchmark data loading performance"""
                  def load_data():
                      # Simulate CSV loading
                      return pd.DataFrame(np.random.randn(10000, 50))
                  
                  result = benchmark(load_data)
                  assert len(result) == 10000
              
              @pytest.mark.benchmark(group="statistics")
              def test_statistics_computation(self, benchmark):
                  """Benchmark statistical computations"""
                  data = np.random.randn(100000, 20)
                  
                  def compute_stats():
                      return {
                          'mean': np.mean(data, axis=0),
                          'std': np.std(data, axis=0),
                          'corr': np.corrcoef(data.T)
                      }
                  
                  result = benchmark(compute_stats)
                  assert 'mean' in result
          
          def test_memory_usage():
              """Test memory usage of critical operations"""
              def process_large_dataset():
                  df = pd.DataFrame(np.random.randn(1000000, 10))
                  return df.describe()
              
              mem_usage = memory_usage(process_large_dataset)
              peak_memory = max(mem_usage)
              
              print(f"Peak memory usage: {peak_memory:.2f} MB")
              assert peak_memory < 1000  # Less than 1GB
          PYTHON
          
      - name: Run Python Benchmarks
        run: |
          # Run pytest benchmarks
          pytest tests/benchmarks/test_performance.py \
            --benchmark-only \
            --benchmark-json=python-benchmarks.json \
            --benchmark-autosave \
            --benchmark-histogram=benchmark-histograms/
            
          # Run memory profiling
          python -m memory_profiler tests/benchmarks/test_performance.py > memory-profile.txt
          
          # Run pyperformance benchmarks
          pyperformance run --output pyperformance-results.json
          
      - name: Analyze Performance
        run: |
          python3 << 'PYTHON'
          import json
          import statistics
          
          # Load benchmark results
          with open('python-benchmarks.json', 'r') as f:
              data = json.load(f)
          
          # Analyze results
          summary = {
              'benchmarks': {},
              'statistics': {}
          }
          
          for benchmark in data.get('benchmarks', []):
              name = benchmark['name']
              stats = benchmark['stats']
              
              summary['benchmarks'][name] = {
                  'mean': stats['mean'],
                  'stddev': stats['stddev'],
                  'min': stats['min'],
                  'max': stats['max'],
                  'iterations': stats['iterations']
              }
          
          # Calculate overall statistics
          all_means = [b['mean'] for b in summary['benchmarks'].values()]
          summary['statistics'] = {
              'total_benchmarks': len(summary['benchmarks']),
              'average_time': statistics.mean(all_means) if all_means else 0,
              'fastest': min(all_means) if all_means else 0,
              'slowest': max(all_means) if all_means else 0
          }
          
          # Save summary
          with open('benchmark-summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          PYTHON
          
      - name: Upload Python Benchmark Results
        uses: actions/upload-artifact@v3
        with:
          name: python-benchmark-results
          path: |
            python-benchmarks.json
            benchmark-summary.json
            memory-profile.txt
            pyperformance-results.json
            benchmark-histograms/
            
  end-to-end-benchmarks:
    name: End-to-End Performance Testing
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Environment
        uses: ./.github/actions/setup-build-env
        with:
          node-version: '18.19.0'
          rust-version: '1.75.0'
          python-version: '3.11'
          
      - name: Build Application
        run: |
          npm ci
          npm run build
          
      - name: Install E2E Test Tools
        run: |
          # Install hyperfine for command-line benchmarking
          wget https://github.com/sharkdp/hyperfine/releases/download/v1.18.0/hyperfine_1.18.0_amd64.deb
          sudo dpkg -i hyperfine_1.18.0_amd64.deb
          
          # Install k6 for load testing
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
          
      - name: Create E2E Benchmark Scripts
        run: |
          # Create test data files
          python3 << 'PYTHON'
          import pandas as pd
          import numpy as np
          
          # Small dataset (1MB)
          small_data = pd.DataFrame(np.random.randn(1000, 50))
          small_data.to_csv('test-data-small.csv', index=False)
          
          # Medium dataset (10MB)
          medium_data = pd.DataFrame(np.random.randn(10000, 50))
          medium_data.to_csv('test-data-medium.csv', index=False)
          
          # Large dataset (100MB)
          large_data = pd.DataFrame(np.random.randn(100000, 50))
          large_data.to_csv('test-data-large.csv', index=False)
          PYTHON
          
          # Create k6 load test
          cat > load-test.js << 'JS'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          
          export let options = {
            stages: [
              { duration: '30s', target: 10 },
              { duration: '1m', target: 50 },
              { duration: '30s', target: 0 },
            ],
            thresholds: {
              http_req_duration: ['p(95)<500'],
              http_req_failed: ['rate<0.1'],
            },
          };
          
          export default function() {
            let response = http.get('http://localhost:1420/api/health');
            check(response, {
              'status is 200': (r) => r.status === 200,
              'response time < 500ms': (r) => r.timings.duration < 500,
            });
            sleep(1);
          }
          JS
          
      - name: Run E2E Benchmarks
        run: |
          # Start the application in background
          npm run tauri dev &
          APP_PID=$!
          
          # Wait for app to start
          sleep 10
          
          # Run benchmarks with different dataset sizes
          hyperfine --warmup 3 --export-json e2e-benchmarks.json \
            --command-name "small-dataset" "curl -X POST http://localhost:1420/api/impute -F 'file=@test-data-small.csv'" \
            --command-name "medium-dataset" "curl -X POST http://localhost:1420/api/impute -F 'file=@test-data-medium.csv'" \
            --command-name "large-dataset" "curl -X POST http://localhost:1420/api/impute -F 'file=@test-data-large.csv'"
          
          # Run load test
          k6 run load-test.js --out json=k6-results.json
          
          # Stop the application
          kill $APP_PID
          
      - name: Upload E2E Benchmark Results
        uses: actions/upload-artifact@v3
        with:
          name: e2e-benchmark-results
          path: |
            e2e-benchmarks.json
            k6-results.json
            
  performance-analysis:
    name: Performance Analysis & Reporting
    needs: [rust-benchmarks, python-benchmarks, end-to-end-benchmarks]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download All Benchmark Results
        uses: actions/download-artifact@v3
        with:
          path: benchmark-artifacts/
          
      - name: Generate Performance Report
        run: |
          python3 << 'PYTHON'
          import json
          import glob
          from datetime import datetime
          
          # Collect all benchmark results
          results = {
              'timestamp': datetime.now().isoformat(),
              'commit': '${{ github.sha }}',
              'branch': '${{ github.ref_name }}',
              'rust': {},
              'python': {},
              'e2e': {}
          }
          
          # Process Rust benchmarks
          rust_files = glob.glob('benchmark-artifacts/rust-benchmark-results/*.json')
          for f in rust_files:
              with open(f, 'r') as file:
                  results['rust'] = json.load(file)
          
          # Process Python benchmarks
          python_files = glob.glob('benchmark-artifacts/python-benchmark-results/*-summary.json')
          for f in python_files:
              with open(f, 'r') as file:
                  results['python'] = json.load(file)
          
          # Process E2E benchmarks
          e2e_files = glob.glob('benchmark-artifacts/e2e-benchmark-results/*.json')
          for f in e2e_files:
              with open(f, 'r') as file:
                  results['e2e'] = json.load(file)
          
          # Generate markdown report
          with open('PERFORMANCE_REPORT.md', 'w') as f:
              f.write(f"# Performance Benchmark Report\n\n")
              f.write(f"**Date**: {results['timestamp']}\n")
              f.write(f"**Commit**: {results['commit'][:7]}\n")
              f.write(f"**Branch**: {results['branch']}\n\n")
              
              f.write("## Summary\n\n")
              
              # Add performance metrics
              if results.get('python', {}).get('statistics'):
                  stats = results['python']['statistics']
                  f.write(f"- **Python Benchmarks**: {stats['total_benchmarks']} tests\n")
                  f.write(f"  - Average execution time: {stats['average_time']:.4f}s\n")
                  f.write(f"  - Fastest: {stats['fastest']:.4f}s\n")
                  f.write(f"  - Slowest: {stats['slowest']:.4f}s\n\n")
              
              # Check for regressions
              regression_detected = False
              # TODO: Compare with baseline and detect regressions
              
              if regression_detected:
                  f.write("⚠️ **Performance regression detected!**\n\n")
              else:
                  f.write("✅ **No performance regressions detected**\n\n")
          
          # Save full results
          with open('performance-results.json', 'w') as f:
              json.dump(results, f, indent=2)
          PYTHON
          
      - name: Upload Performance Report
        uses: actions/upload-artifact@v3
        with:
          name: performance-report
          path: |
            PERFORMANCE_REPORT.md
            performance-results.json
            
      - name: Comment PR with Performance Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('PERFORMANCE_REPORT.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });