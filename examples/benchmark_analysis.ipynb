{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirImpute Pro Benchmark Analysis\n",
    "\n",
    "This notebook demonstrates comprehensive benchmark analysis using AirImpute Pro's academic-grade benchmarking framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from airimpute.benchmarking import (\n",
    "    BenchmarkRunner, \n",
    "    BenchmarkDatasetManager,\n",
    "    PerformanceMetrics,\n",
    "    StatisticalTesting\n",
    ")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset manager\n",
    "manager = BenchmarkDatasetManager()\n",
    "\n",
    "# Create synthetic datasets with varying characteristics\n",
    "dataset_configs = [\n",
    "    {'name': 'low_missing', 'missing_rate': 0.1, 'pattern': 'random'},\n",
    "    {'name': 'medium_missing', 'missing_rate': 0.2, 'pattern': 'blocks'},\n",
    "    {'name': 'high_missing', 'missing_rate': 0.3, 'pattern': 'temporal'},\n",
    "    {'name': 'mixed_pattern', 'missing_rate': 0.25, 'pattern': 'all'}\n",
    "]\n",
    "\n",
    "for config in dataset_configs:\n",
    "    manager.create_synthetic_dataset(\n",
    "        name=config['name'],\n",
    "        n_timesteps=5000,\n",
    "        n_stations=20,\n",
    "        missing_rate=config['missing_rate'],\n",
    "        pattern=config['pattern'],\n",
    "        temporal_correlation=0.8,\n",
    "        spatial_correlation=0.6,\n",
    "        seed=42\n",
    "    )\n",
    "    print(f\"Created dataset: {config['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Missing Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, config in enumerate(dataset_configs):\n",
    "    dataset = manager.get_dataset(config['name'])\n",
    "    data = dataset.data.values[:500, :10]  # First 500 time steps, 10 stations\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    mask = np.isnan(data)\n",
    "    ax.imshow(mask.T, aspect='auto', cmap='RdBu', interpolation='none')\n",
    "    ax.set_title(f\"{config['name']} (Missing: {config['missing_rate']*100:.0f}%)\")\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Station')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.suptitle('Missing Data Patterns', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import imputation methods\n",
    "from airimpute.methods.simple import MeanImputation, ForwardFillImputation\n",
    "from airimpute.methods.interpolation import LinearInterpolation, SplineInterpolation\n",
    "from airimpute.methods.machine_learning import RandomForestImputation, XGBoostImputation\n",
    "from airimpute.methods.rah import RobustAdaptiveHierarchicalImputation\n",
    "\n",
    "# Define methods\n",
    "methods = {\n",
    "    'Mean': MeanImputation(),\n",
    "    'Forward Fill': ForwardFillImputation(),\n",
    "    'Linear': LinearInterpolation(),\n",
    "    'Spline': SplineInterpolation(order=3),\n",
    "    'Random Forest': RandomForestImputation(n_estimators=50, max_depth=10),\n",
    "    'XGBoost': XGBoostImputation(n_estimators=50, learning_rate=0.1),\n",
    "    'RAH': RobustAdaptiveHierarchicalImputation(n_estimators=30)\n",
    "}\n",
    "\n",
    "# Convert to callable format\n",
    "method_callables = {name: method.impute for name, method in methods.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize benchmark runner\n",
    "runner = BenchmarkRunner(\n",
    "    dataset_manager=manager,\n",
    "    use_gpu=True,  # Enable GPU if available\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Run benchmark\n",
    "print(\"Running benchmark... This may take a few minutes.\")\n",
    "results = runner.run_benchmark(\n",
    "    methods=method_callables,\n",
    "    datasets=[config['name'] for config in dataset_configs],\n",
    "    cv_splits=5,\n",
    "    save_predictions=False,\n",
    "    parallel=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBenchmark completed! Evaluated {len(results)} configurations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_data = []\n",
    "for result in results:\n",
    "    results_data.append({\n",
    "        'Method': result.method_name,\n",
    "        'Dataset': result.dataset_name,\n",
    "        'RMSE': result.metrics.get('rmse', np.nan),\n",
    "        'MAE': result.metrics.get('mae', np.nan),\n",
    "        'R²': result.metrics.get('r2', np.nan),\n",
    "        'Runtime': result.runtime,\n",
    "        'Memory': result.memory_usage\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Summary statistics\n",
    "summary = results_df.groupby('Method')[['RMSE', 'MAE', 'R²', 'Runtime']].agg(['mean', 'std'])\n",
    "summary = summary.round(4)\n",
    "print(\"\\nMethod Performance Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "ax = axes[0, 0]\n",
    "pivot_rmse = results_df.pivot(index='Method', columns='Dataset', values='RMSE')\n",
    "pivot_rmse.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('RMSE by Method and Dataset')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.legend(title='Dataset', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE comparison\n",
    "ax = axes[0, 1]\n",
    "pivot_mae = results_df.pivot(index='Method', columns='Dataset', values='MAE')\n",
    "pivot_mae.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('MAE by Method and Dataset')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.legend(title='Dataset', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Runtime comparison\n",
    "ax = axes[1, 0]\n",
    "runtime_avg = results_df.groupby('Method')['Runtime'].mean().sort_values()\n",
    "runtime_avg.plot(kind='barh', ax=ax, color='coral')\n",
    "ax.set_title('Average Runtime by Method')\n",
    "ax.set_xlabel('Runtime (seconds)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# R² scatter plot\n",
    "ax = axes[1, 1]\n",
    "for method in results_df['Method'].unique():\n",
    "    method_data = results_df[results_df['Method'] == method]\n",
    "    ax.scatter(method_data['Runtime'], method_data['R²'], \n",
    "               label=method, s=100, alpha=0.7)\n",
    "ax.set_xlabel('Runtime (seconds)')\n",
    "ax.set_ylabel('R²')\n",
    "ax.set_title('Runtime vs Accuracy Trade-off')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for statistical testing\n",
    "method_scores = {}\n",
    "for method in results_df['Method'].unique():\n",
    "    scores = results_df[results_df['Method'] == method]['RMSE'].values\n",
    "    method_scores[method] = scores.tolist()\n",
    "\n",
    "# Perform Friedman test\n",
    "friedman_result = StatisticalTesting.friedman_test(method_scores)\n",
    "\n",
    "print(\"\\nStatistical Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Friedman Test:\")\n",
    "print(f\"  χ² statistic: {friedman_result['statistic']:.4f}\")\n",
    "print(f\"  p-value: {friedman_result['pvalue']:.4f}\")\n",
    "print(f\"  Significant: {friedman_result['significant']}\")\n",
    "\n",
    "if friedman_result['significant']:\n",
    "    print(\"\\nMethod Rankings (lower is better):\")\n",
    "    for method, rank in sorted(friedman_result['method_ranks'].items(), \n",
    "                               key=lambda x: x[1]):\n",
    "        print(f\"  {rank:.2f}: {method}\")\n",
    "    \n",
    "    print(\"\\nPost-hoc Analysis (Nemenyi test):\")\n",
    "    print(\"Significant differences:\")\n",
    "    for method1, comparisons in friedman_result['post_hoc'].items():\n",
    "        for method2, is_different in comparisons.items():\n",
    "            if is_different and method1 < method2:  # Avoid duplicates\n",
    "                print(f\"  {method1} ≠ {method2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance heatmap\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# RMSE heatmap\n",
    "pivot_rmse = results_df.pivot(index='Method', columns='Dataset', values='RMSE')\n",
    "sns.heatmap(pivot_rmse, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax1)\n",
    "ax1.set_title('RMSE Performance Heatmap')\n",
    "\n",
    "# R² heatmap  \n",
    "pivot_r2 = results_df.pivot(index='Method', columns='Dataset', values='R²')\n",
    "sns.heatmap(pivot_r2, annot=True, fmt='.4f', cmap='RdYlGn', ax=ax2)\n",
    "ax2.set_title('R² Performance Heatmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Method Ranking Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive ranking\n",
    "metrics_to_rank = ['RMSE', 'MAE', 'Runtime']\n",
    "rankings = {}\n",
    "\n",
    "for metric in metrics_to_rank:\n",
    "    if metric == 'R²':  # Higher is better\n",
    "        ranks = results_df.groupby('Method')[metric].mean().rank(ascending=False)\n",
    "    else:  # Lower is better\n",
    "        ranks = results_df.groupby('Method')[metric].mean().rank()\n",
    "    rankings[metric] = ranks\n",
    "\n",
    "# Create ranking DataFrame\n",
    "ranking_df = pd.DataFrame(rankings)\n",
    "ranking_df['Average Rank'] = ranking_df.mean(axis=1)\n",
    "ranking_df = ranking_df.sort_values('Average Rank')\n",
    "\n",
    "print(\"\\nComprehensive Method Ranking:\")\n",
    "print(\"=\" * 50)\n",
    "print(ranking_df.round(2))\n",
    "\n",
    "# Best method overall\n",
    "best_method = ranking_df.index[0]\n",
    "print(f\"\\nBest performing method overall: {best_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results for publication\n",
    "output_dir = Path('benchmark_outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save CSV\n",
    "results_df.to_csv(output_dir / 'benchmark_results.csv', index=False)\n",
    "print(f\"Results saved to {output_dir / 'benchmark_results.csv'}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary.to_csv(output_dir / 'benchmark_summary.csv')\n",
    "print(f\"Summary saved to {output_dir / 'benchmark_summary.csv'}\")\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = pivot_rmse.to_latex(\n",
    "    float_format='%.4f',\n",
    "    caption='RMSE performance across methods and datasets',\n",
    "    label='tab:rmse_results'\n",
    ")\n",
    "\n",
    "with open(output_dir / 'rmse_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "print(f\"LaTeX table saved to {output_dir / 'rmse_table.tex'}\")\n",
    "\n",
    "# Save reproducibility information\n",
    "repro_info = {\n",
    "    'benchmark_id': runner.reproducibility_info.benchmark_id,\n",
    "    'timestamp': runner.reproducibility_info.timestamp.isoformat(),\n",
    "    'platform': runner.reproducibility_info.platform_info,\n",
    "    'random_seed': runner.random_seed,\n",
    "    'n_methods': len(methods),\n",
    "    'n_datasets': len(dataset_configs),\n",
    "    'cv_splits': 5\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(output_dir / 'reproducibility_info.json', 'w') as f:\n",
    "    json.dump(repro_info, f, indent=2)\n",
    "print(f\"Reproducibility info saved to {output_dir / 'reproducibility_info.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Creating diverse benchmark datasets\n",
    "2. Running comprehensive method comparisons\n",
    "3. Statistical testing for significance\n",
    "4. Visualizing results effectively\n",
    "5. Exporting publication-ready outputs\n",
    "\n",
    "The results show clear performance differences between methods, with advanced ML methods (XGBoost, RAH) generally outperforming simpler approaches, though at higher computational cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}